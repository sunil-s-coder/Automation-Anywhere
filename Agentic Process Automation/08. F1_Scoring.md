# Testing AI Accuracy with F1 Scores

## Why Testing AI is Hard?

- **Legacy automation:** Simple pass & fail criteria.
- **AI testing challenges:**
  1. **Test data:** Often unstructured. The same data can be rearranged in infinite ways.
  2. **Output:**
     - Not binary (i.e., not just pass or fail).
     - Output may be partially correct or "close enough" rather than exact.

---

## AI Tests

### 1. F1 Scoring

Used to measure accuracy in scenarios where the output must match exact information. Applicable in:

- Classification
- Sentiment Analysis
- Named Entity Recognition (NER) & Extraction
- Data Transformation
- Decision Making

### 2. Completeness & Faithfulness

- **Completeness:** Measures whether all information requested in the prompt is included in the output.
- **Faithfulness:** Measures whether the information provided in the output is relevant and accurate.

Used in:

- Language Translation
- Summarization
- Content Generation

---

## F1 Scoring

### Definition

1. F1 scoring measures how well the model's predicted labels match the actual labels.
2. It evaluates:
   - **Precision:** How often the predictions are correct.
   - **Recall:** How many correct predictions were made out of all possible correct predictions.

---

### Steps to Measure F1 Scoring

#### 1. Build a Ground Truth

- **Collect the test data**
- **Define the output types:**
  - **Classification:** Categories the model should classify into.
  - **Sentiment Analysis:** Emotions the model needs to identify.
  - **NER & Extraction:** Specific fields to be extracted.
- **Manually label the data**
- **Quality check the labeled data**
- **Reserve validation data**

#### 2. Compile Results

- **Test the AI model**
- **Record the output:**
  - Compare **actual vs. predicted output** (ideally in tabular format)
  - Measure:
    - **True Positive (TP):** Correctly predicted
    - **False Positive (FP):** Incorrectly predicted
    - **False Negative (FN):** Not predicted or missed
- **Calculate metrics:**
  - `Precision = TP / (TP + FP)`
  - `Recall = TP / (TP + FN)`
  - `F1 Score = (2 * Precision * Recall) / (Precision + Recall)`

#### 3. Evaluate Performance

- Assess F1 score:
  - `> 0.8` → Good
  - `< 0.8` → Needs improvement (especially for critical use cases)
- Assess:
  - Types of prediction problems
  - Business success or impact of the model

#### 4. Prompt Refinement (as needed)

- Rebuild the prompt
- Retest the model
- Validate using reserved validation data

---
